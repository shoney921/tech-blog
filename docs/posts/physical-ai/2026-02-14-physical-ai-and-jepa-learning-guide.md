---
title: "피지컬 AI 기초부터 JEPA까지 — 단계별 학습 가이드"
date: 2026-02-14T10:00:00
---

# 피지컬 AI 기초부터 JEPA까지 — 단계별 학습 가이드

> AI가 텍스트를 생성하던 시대를 넘어, 현실 세계에서 직접 행동하는 **피지컬 AI** 시대가 열리고 있다. 이 글에서는 피지컬 AI의 개념부터 핵심 기술인 **JEPA(Joint Embedding Predictive Architecture)** 까지 차근차근 살펴본다.

---

## 1단계: 피지컬 AI란 무엇인가?

### 정의

**피지컬 AI(Physical AI)** 는 현실 세계를 이해하고, 추론하며, 물리적으로 행동하는 AI를 말한다. 기존의 생성형 AI가 텍스트와 이미지를 화면 안에서 '생성'했다면, 피지컬 AI는 실제 세계에서 '행동'한다.

쉽게 말해 **'몸을 가진 뇌(Brain in a Body)'** 다. 문을 열기 위해 손잡이의 모양과 재질을 **보고**, 얼마나 힘을 주어 돌려야 할지 **판단**하고, 로봇 팔로 직접 **행동**하는 것이 피지컬 AI다.

### 기존 AI와의 차이

| 구분 | 생성형 AI (ChatGPT 등) | 피지컬 AI |
|------|----------------------|-----------|
| 활동 영역 | 디지털 세계 (텍스트, 이미지) | 물리적 세계 (로봇, 자율주행차) |
| 핵심 능력 | 언어 이해·생성 | 환경 인지 + 판단 + 물리적 행동 |
| 상호작용 | 사용자와 대화 | 현실 세계와 직접 상호작용 |
| 학습 데이터 | 텍스트, 이미지 | 영상, 센서, 시뮬레이션 데이터 |

### 관련 용어 정리

- **임바디드 AI(Embodied AI):** '몸을 가진 AI'. 학계에서 환경과의 상호작용을 통한 학습을 강조할 때 주로 사용한다.
- **로보틱 AI(Robotic AI):** AI가 탑재된 로봇. 피지컬 AI의 가장 대표적인 구현 형태다.
- **월드 모델(World Model):** AI가 세상의 물리 법칙과 인과관계를 내부적으로 모델링한 것. 피지컬 AI의 핵심 요소다.

---

## 2단계: 왜 지금 피지컬 AI인가?

피지컬 AI가 갑자기 부상한 것이 아니다. 여러 기술이 동시에 성숙기에 접어들면서 가능해졌다.

### 세 가지 기술적 배경

**1. 생성형 AI의 폭발적 발전**

ChatGPT 이후 2년간 LLM이 빠르게 진화했다. 이 '두뇌'를 로봇에 탑재하면서, 로봇이 상황을 이해하고 스스로 판단하는 것이 가능해졌다.

**2. 시뮬레이션 기술의 성숙**

디지털 트윈으로 가상 환경에서 수백만 번 훈련 후 검증된 모델만 현장에 투입하는 프로세스가 자리 잡았다.

**3. 하드웨어 비용 하락**

로봇 하드웨어, 센서, 컴퓨팅 인프라 가격이 현실적인 수준으로 내려왔다.

### 젠슨 황의 선언

NVIDIA CEO 젠슨 황은 CES 2025에서 "AI는 현실 세계를 이해하고, 상호작용하며, 물리적인 일을 수행하는 **피지컬 AI의 시대**로 접어들고 있다"고 선언했다. CES 2026에서도 피지컬 AI가 핵심 키워드로 자리 잡으며, AI 혁신의 중심이 소프트웨어를 넘어 물리 시스템으로 이동하고 있음을 확인했다.

---

## 3단계: 피지컬 AI 핵심 기술 스택 이해하기

피지컬 AI를 구현하려면 여러 기술이 유기적으로 결합되어야 한다. 전체 기술 스택을 이해해 보자.

### 인지(Perception)

로봇이 카메라, LiDAR, 센서 등으로 주변 환경을 인식하는 단계다. 객체 인식, 깊이 추정, 장면 이해 등이 여기에 해당한다.

### 월드 모델(World Model)

인지한 정보를 바탕으로 세상이 어떻게 동작하는지 내부적으로 시뮬레이션하는 모델이다. "이 물체를 밀면 어떻게 될까?", "이 길로 가면 어디에 도착할까?"를 예측한다. **JEPA가 바로 이 월드 모델을 구축하기 위한 핵심 아키텍처다.**

### 계획(Planning)

월드 모델의 예측을 기반으로 목표를 달성하기 위한 행동 순서를 결정한다.

### 행동(Action)

계획에 따라 로봇 팔, 바퀴 등 물리적 액추에이터를 제어해 실제 행동을 수행한다.

```
인지(Perception) → 월드 모델(World Model) → 계획(Planning) → 행동(Action)
       ↑                                                          |
       └──────────────── 피드백 루프 ────────────────────────────────┘
```

---

## 4단계: JEPA — 월드 모델의 핵심 아키텍처

### JEPA란?

**JEPA(Joint Embedding Predictive Architecture)** 는 Meta의 수석 AI 과학자 **얀 르쿤(Yann LeCun)** 이 제안한 자기지도학습 프레임워크다. 2022년 발표한 논문 *"A Path Towards Autonomous Machine Intelligence"* 에서 인간 수준의 자율적 AI를 달성하기 위한 청사진으로 제시했다.

### 핵심 아이디어: "추상적 공간에서 예측하라"

기존 생성형 AI와 JEPA의 가장 큰 차이는 **예측 방식**에 있다.

| 구분 | 생성형 AI | JEPA |
|------|----------|------|
| 예측 대상 | 다음 토큰, 다음 픽셀 (원본 데이터 공간) | 추상적 표현(임베딩) 공간 |
| 예측 방식 | "다음 프레임은 어떻게 생겼을까?" | "다음 프레임의 **의미**는 무엇일까?" |
| 장점 | 세밀한 생성 가능 | 핵심 의미에 집중, 불필요한 디테일 무시 |

예를 들어, 사람이 컵을 들어올리는 영상을 본다고 하자.

- **생성형 AI:** 다음 프레임의 모든 픽셀 — 배경의 나뭇잎, 그림자, 조명 변화까지 — 전부 예측하려 한다.
- **JEPA:** "컵이 위로 올라가고 있다"는 **추상적 의미**만 예측한다. 배경의 사소한 변화는 무시한다.

이것이 왜 중요한가? 인간이 세상을 이해하는 방식과 동일하기 때문이다. 우리는 주변 환경의 모든 픽셀을 기억하지 않지만, "컵이 올라갔다"는 핵심 개념은 정확히 파악한다.

### JEPA의 구조

JEPA는 크게 세 가지 구성요소로 이루어진다.

```
┌─────────────────────────────────────────────────┐
│                   JEPA 아키텍처                    │
│                                                   │
│   컨텍스트 입력        타겟 입력                     │
│       │                   │                       │
│       ▼                   ▼                       │
│  ┌──────────┐      ┌──────────┐                  │
│  │ 컨텍스트  │      │  타겟    │                   │
│  │ 인코더   │      │ 인코더   │                    │
│  └────┬─────┘      └────┬─────┘                  │
│       │                  │                        │
│       ▼                  │                        │
│  ┌──────────┐           │                        │
│  │ 프리딕터  │───비교────│                        │
│  │(Predictor)│          │                        │
│  └──────────┘           ▼                        │
│                   손실 함수로 학습                  │
└─────────────────────────────────────────────────┘
```

1. **컨텍스트 인코더(Context Encoder):** 보이는 부분(컨텍스트)을 추상적 표현으로 변환
2. **타겟 인코더(Target Encoder):** 가려진 부분(타겟)을 추상적 표현으로 변환
3. **프리딕터(Predictor):** 컨텍스트의 표현으로부터 타겟의 표현을 예측

학습 목표는 프리딕터가 예측한 표현과 타겟 인코더가 만든 실제 표현 사이의 차이를 줄이는 것이다.

---

## 5단계: JEPA 패밀리 — I-JEPA에서 V-JEPA 2까지

JEPA는 개념적 프레임워크이고, 이를 구체적으로 구현한 모델들이 있다. 단계적으로 발전해 온 과정을 살펴보자.

### I-JEPA (Image-based JEPA) — 이미지로 시작하기

**I-JEPA**는 JEPA를 이미지에 적용한 첫 번째 구현체다 (2023년 공개).

**작동 방식:**
1. 이미지를 여러 **패치(patch)** 로 분할
2. 일부 패치를 **마스킹**(가림)
3. 보이는 패치(컨텍스트)로부터 가려진 패치(타겟)의 **추상적 표현**을 예측

Vision Transformer(ViT)를 기반으로 하며, 라벨 없이 순수하게 자기지도학습으로 강력한 이미지 표현을 학습한다. ViT-Huge/14 모델을 ImageNet에서 A100 GPU 16개로 72시간 이내에 훈련하여 우수한 성능을 달성했다.

**의의:** 데이터 증강(augmentation) 없이도 의미론적으로 풍부한 표현을 학습할 수 있음을 입증했다. I-JEPA의 프리딕터는 정적 이미지에서의 공간적 불확실성을 모델링하는 **원시적 월드 모델**로 볼 수 있다.

### V-JEPA (Video-based JEPA) — 영상으로 확장하기

**V-JEPA**는 JEPA를 비디오에 적용한 모델이다 (2024년 공개).

**핵심 차이:** 이미지가 공간적(2D) 마스킹만 했다면, 비디오는 **시공간(spatiotemporal) 마스킹**을 한다. 시간축과 공간축 모두에서 큰 영역을 가려야 모델이 의미 있는 학습을 한다.

왜 시공간 마스킹이 필요한가? 대부분의 영상에서 사물은 시간에 따라 천천히 변한다. 특정 시점만 가리고 인접 프레임을 보여주면 너무 쉬운 과제가 되어 모델이 깊이 있는 이해를 하지 못한다.

**Frozen Evaluation:** V-JEPA는 사전 학습된 인코더와 프리딕터를 동결(freeze)한 채, 작은 경량 레이어만 추가해 새로운 과제에 적응하는 방식으로 평가할 수 있다. 이는 매우 효율적이다.

**의의:** V-JEPA의 프리딕터는 **초기 형태의 물리 월드 모델**이다. 프레임의 모든 것을 보지 않아도 개념적으로 무슨 일이 일어나고 있는지 파악할 수 있다.

### V-JEPA 2 — 물리 세계를 이해하는 월드 모델

**V-JEPA 2**는 2025년 6월 Meta가 공개한 최신 모델로, 피지컬 AI의 핵심 모듈인 월드 모델로서의 역할을 본격적으로 수행한다.

**세 가지 핵심 능력:**

| 능력 | 설명 |
|------|------|
| **이해(Understanding)** | 영상 속 객체, 동작, 움직임 인식 |
| **예측(Prediction)** | 세계가 어떻게 변할지, 특정 행동의 결과 예측 |
| **계획(Planning)** | 예측을 기반으로 목표 달성을 위한 행동 순서 수립 |

**2단계 학습 과정:**

```
1단계: 액션 없는 사전학습 (Actionless Pre-training)
  └─ 100만 시간 이상의 영상 + 100만 장 이미지
  └─ 사람과 물체의 상호작용, 물리적 움직임 학습

2단계: 액션 조건부 후학습 (Action-Conditioned Post-Training)
  └─ 62시간 미만의 비라벨 로봇 영상 (Droid 데이터셋)
  └─ "이 행동을 하면 세상이 어떻게 변할까?"를 학습
  └─ → V-JEPA 2-AC 모델 생성
```

**주요 성과:**
- Something-Something v2에서 77.3% top-1 정확도 (동작 이해)
- Epic-Kitchens-100에서 39.7 recall-at-5 (행동 예측, SOTA)
- 새로운 환경에서 물체 집고 놓기 성공률 65%~80%
- NVIDIA Cosmos 대비 최대 30배 빠른 속도

**로봇 적용:** V-JEPA 2-AC는 Franka 로봇 팔에 **제로샷(zero-shot)** 으로 배치되어, 해당 환경의 데이터 수집이나 태스크별 훈련 없이도 이미지 목표를 활용한 물체 조작이 가능하다.

**오픈소스:** [GitHub](https://github.com/facebookresearch/vjepa2)에서 코드와 모델 체크포인트를 공개하고 있다.

---

## 6단계: JEPA가 LLM과 다른 점 — 왜 새로운 접근이 필요한가?

현재 LLM(Large Language Model)은 놀라운 성과를 거두고 있지만, 얀 르쿤은 LLM만으로는 진정한 지능에 도달할 수 없다고 주장한다.

### LLM의 한계

1. **텍스트만으로 세상을 이해할 수 없다:** 아이는 태어나서 수년간 시각, 촉각, 청각 등 다감각 경험으로 세상을 배운다. 텍스트는 세상 정보의 극히 일부에 불과하다.
2. **토큰 단위 예측의 비효율성:** 다음 토큰을 예측하는 방식은 모든 디테일에 동일한 가중치를 둔다. 중요한 것과 중요하지 않은 것을 구분하지 못한다.
3. **물리적 추론 부재:** "유리컵을 바닥에 떨어뜨리면?"이라는 질문에 LLM은 텍스트 패턴으로 답하지만, 물리 법칙을 이해하고 있는 것은 아니다.

### JEPA의 해법

| LLM의 한계 | JEPA의 접근 |
|-----------|------------|
| 텍스트 공간에서만 예측 | 추상적 표현 공간에서 예측 |
| 모든 디테일에 동일 가중치 | 핵심 의미에 집중, 불필요한 디테일 무시 |
| 순차적 토큰 생성 | 에너지 기반 모델로 복잡한 의존 관계 포착 |
| 반응적(reactive) 응답 | 계획(planning)과 추론 지원 |

---

## 7단계: 피지컬 AI 개발 플랫폼 이해하기

JEPA 같은 월드 모델을 실제 로봇에 적용하려면 개발 플랫폼이 필요하다. 현재 가장 주목받는 플랫폼을 살펴보자.

### NVIDIA 피지컬 AI 스택

NVIDIA는 피지컬 AI를 위한 풀스택 솔루션을 제공한다.

| 구성요소 | 역할 |
|---------|------|
| **Omniverse** | 3D 시뮬레이션 플랫폼 (OpenUSD 기반) |
| **Isaac Sim** | 로봇 시뮬레이션 및 합성 데이터 생성 |
| **Isaac Lab** | 강화학습, 모방학습, 모션 플래닝 통합 프레임워크 |
| **Cosmos** | 월드 파운데이션 모델 — 합성 데이터 대규모 생성 |
| **Newton** | 물리 엔진 (Google DeepMind, Disney Research 공동 개발) |
| **Jetson** | 로봇용 엣지 컴퓨팅 프로세서 |

**워크플로:** Isaac Sim에서 시뮬레이션 → Cosmos로 합성 데이터 증강 → Isaac Lab에서 학습 → Jetson에 배포

### 주요 기업 동향

- **테슬라 옵티머스:** 자체 공장 투입으로 실전 데이터 축적 중, 2026년 대량 생산 목표
- **피규어 AI:** BMW 공장에 로봇 상용 투입
- **보스턴 다이내믹스:** 전동식 Atlas 공개, 2028년 실전 투입 로드맵
- **Meta:** V-JEPA 2 오픈소스 공개로 월드 모델 생태계 선도

---

## 8단계: 학습 로드맵 — 어디서부터 시작할까?

피지컬 AI와 JEPA를 학습하기 위한 단계별 로드맵을 제안한다.

### Phase 1: 기초 지식 쌓기

- **딥러닝 기초:** CNN, Transformer, 어텐션 메커니즘
- **컴퓨터 비전:** 이미지 분류, 객체 탐지, 세그멘테이션
- **자기지도학습(Self-Supervised Learning):** 대조학습(Contrastive Learning), 마스크드 오토인코더(MAE) 개념 이해
- **추천 자료:** Stanford CS231n, Fast.ai 강좌

### Phase 2: JEPA 이론 이해

- **필수 논문:** Yann LeCun, *"A Path Towards Autonomous Machine Intelligence"* (2022)
- **I-JEPA 논문:** *"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"* (CVPR 2023)
- **에너지 기반 모델(EBM):** JEPA의 이론적 토대
- **Vision Transformer(ViT):** I-JEPA, V-JEPA의 백본 아키텍처

### Phase 3: 코드로 실습하기

- **I-JEPA 구현 체험:** [facebookresearch/ijepa](https://github.com/facebookresearch/ijepa) GitHub 레포
- **V-JEPA 2 코드 분석:** [facebookresearch/vjepa2](https://github.com/facebookresearch/vjepa2) GitHub 레포
- **환경:** PyTorch + GPU 환경 (Google Colab Pro 또는 로컬 GPU)

### Phase 4: 시뮬레이션 환경 구축

- **NVIDIA Isaac Sim:** 로봇 시뮬레이션 시작하기
- **Isaac Lab:** 강화학습 기반 로봇 훈련
- **MuJoCo / PyBullet:** 경량 물리 시뮬레이션으로 먼저 실습

### Phase 5: 통합 프로젝트

- 간단한 로봇 팔 제어 환경에서 JEPA 기반 월드 모델 구축
- 시뮬레이션에서 학습 → 실제 로봇으로 전이(Sim-to-Real Transfer)
- 학습 과정 블로그나 논문으로 정리

---

## 마무리: 피지컬 AI의 미래

골드만삭스는 휴머노이드 로봇 시장이 2035년 380억 달러 규모로 성장할 것으로 전망했다. AI 로보틱스 시장은 2030년까지 연평균 38.5% 성장이 예상된다.

2026년은 피지컬 AI가 실험실을 벗어나 현실로 스며드는 원년이 될 전망이다. JEPA로 대표되는 월드 모델 기술은 이 전환의 핵심 동력이다.

텍스트를 생성하던 AI가 이제 물리적 노동을 수행하는 단계로 진화했다. 국가 경쟁력은 '누가 더 똑똑한 AI를 가졌는가'를 넘어 **'누가 더 잘 움직이는 AI를 현실에 구현하는가'** 로 결정될 것이다.

지금이 피지컬 AI를 학습하기에 가장 좋은 시점이다. 기초부터 차근차근, JEPA의 원리를 이해하고 직접 코드를 만져보며 이 거대한 흐름에 올라타 보자.

---

## 참고 자료

- [NVIDIA Physical AI Glossary](https://www.nvidia.com/en-us/glossary/generative-physical-ai/)
- [Meta AI - V-JEPA 2 소개](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)
- [Meta AI - I-JEPA 소개](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/)
- [Meta AI - V-JEPA 소개](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/)
- [V-JEPA 2 GitHub](https://github.com/facebookresearch/vjepa2)
- [I-JEPA GitHub](https://github.com/facebookresearch/ijepa)
- [슈퍼브에이아이 - 피지컬 AI란 무엇인가?](https://blog-ko.superb-ai.com/what-is-physical-ai-next-wave/)
- [NVIDIA Isaac Sim](https://developer.nvidia.com/isaac/sim)
- [NVIDIA 피지컬 AI 블로그](https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/)
- [Yann LeCun, "A Path Towards Autonomous Machine Intelligence" (2022)](https://openreview.net/pdf?id=BZ5a1r-kVsf)
